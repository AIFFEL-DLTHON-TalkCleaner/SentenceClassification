{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **필요한 데이터셋**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\서우\\Desktop\\GitHub\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\서우\\Desktop\\GitHub\\.venv\\Lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\서우\\Desktop\\GitHub\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "import transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kobert_tokenizer import KoBERTTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 후보였던 모델 1..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\서우\\Desktop\\GitHub\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\서우\\.cache\\huggingface\\hub\\models--gogamza--kobart-base-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "loading file tokenizer.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--gogamza--kobart-base-v1\\snapshots\\5aa5969f3c0b5df5ab02be4f747bf3601c4fdb61\\tokenizer.json\n",
      "loading file added_tokens.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--gogamza--kobart-base-v1\\snapshots\\5aa5969f3c0b5df5ab02be4f747bf3601c4fdb61\\added_tokens.json\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--gogamza--kobart-base-v1\\snapshots\\5aa5969f3c0b5df5ab02be4f747bf3601c4fdb61\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at None\n",
      "c:\\Users\\서우\\Desktop\\GitHub\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--gogamza--kobart-base-v1\\snapshots\\5aa5969f3c0b5df5ab02be4f747bf3601c4fdb61\\config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"gogamza/kobart-base-v1\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 1.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--gogamza--kobart-base-v1\\snapshots\\5aa5969f3c0b5df5ab02be4f747bf3601c4fdb61\\config.json\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "Model config BartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 1.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--gogamza--kobart-base-v1\\snapshots\\5aa5969f3c0b5df5ab02be4f747bf3601c4fdb61\\model.safetensors\n",
      "All model checkpoint weights were used when initializing BartModel.\n",
      "\n",
      "All the weights of BartModel were initialized from the model checkpoint at gogamza/kobart-base-v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast, BartModel\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('gogamza/kobart-base-v1')\n",
    "\n",
    "# 모델 로드\n",
    "model = BartModel.from_pretrained('gogamza/kobart-base-v1')\n",
    "\n",
    "toke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15668, 9517, 9517, 15673, 14301, 10513, 9698, 17826, 27178, 10496, 14426, 14850, 11803]\n",
      "['▁보는', '내', '내', '▁그대로', '▁들어', '맞', '는', '▁예측', '▁카리스', '마', '▁없는', '▁악', '역']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))\n",
    "print(tokenizer.tokenize(\"보는내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 후보였던 모델 2.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\서우\\Desktop\\GitHub\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kobert\",\n",
      "  \"architectures\": [\n",
      "    \"BertModelForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kobert\",\n",
      "  \"architectures\": [\n",
      "    \"BertModelForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at beomi/kobert were not used when initializing BertForMaskedLM: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at beomi/kobert and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generation config file not found, using a generation config created from the model config.\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kobert\",\n",
      "  \"architectures\": [\n",
      "    \"BertModelForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kobert\",\n",
      "  \"architectures\": [\n",
      "    \"BertModelForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kobert\",\n",
      "  \"architectures\": [\n",
      "    \"BertModelForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"beomi/kobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kobert\",\n",
      "  \"architectures\": [\n",
      "    \"BertModelForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kobert\",\n",
      "  \"architectures\": [\n",
      "    \"BertModelForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kobert\",\n",
      "  \"architectures\": [\n",
      "    \"BertModelForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"beomi/kobert\",\n",
      "  \"architectures\": [\n",
      "    \"BertModelForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--kobert\\snapshots\\a9108e99ab5e3f88d767a4c1a0340716d918dd7e\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at beomi/kobert were not used when initializing BertForMaskedLM: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at beomi/kobert and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generation config file not found, using a generation config created from the model config.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kobert\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"beomi/kobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 3]\n",
      "['[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"보는 내내 그대로 들어맞는 예측 카리스마 없는 악역\"))\n",
    "print(tokenizer.tokenize(\"보는 내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정한 모델_KcBERT-v2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\서우\\Desktop\\GitHub\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"beomi/KcBERT-v2023\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"beomi/KcBERT-v2023\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at beomi/KcBERT-v2023.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "loading file vocab.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"beomi/KcBERT-v2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"beomi/KcBERT-v2023\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at beomi/KcBERT-v2023.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#이걸로 결정 !!!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcBERT-v2023\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"beomi/KcBERT-v2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2648, 8737, 3246, 1070, 4403, 11522, 2134, 23716, 995, 1314, 690, 2]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"보는 내내 그대로 들어맞는 예측 카리스마 없는 악역\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **실제로 모델 만들기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Custom dataset 구성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "    \n",
    "        encoding = self.tokenizer(text, padding = 'max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        input_ids = encoding['input_ids'].squeeze() #차원 줄이기 \n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "    \n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:/Users/서우/Desktop/GitHub/merged_df_4000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['일반 대화', '기타 괴롭힘 대화', '직장 내 괴롭힘 대화', '협박 대화', '갈취 대화'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\서우\\AppData\\Local\\Temp\\ipykernel_6792\\3352899884.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['labels'] = data['class'].replace(category_to_number)\n"
     ]
    }
   ],
   "source": [
    "category_to_number = {\n",
    "    '일반 대화': 0,\n",
    "    '기타 괴롭힘 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '협박 대화': 3,\n",
    "    '갈취 대화': 4\n",
    "}\n",
    "data['labels'] = data['class'].replace(category_to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "7945    0\n",
       "7946    0\n",
       "7947    0\n",
       "7948    0\n",
       "7949    0\n",
       "Name: labels, Length: 7950, dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       1\n",
       "       ..\n",
       "7945    0\n",
       "7946    0\n",
       "7947    0\n",
       "7948    0\n",
       "7949    0\n",
       "Name: labels, Length: 7950, dtype: int64"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        휴일이니...쉬게해주세요내말이 ... ㅜㅜ 너무 가기 싫어\n",
       "1       물고기 잡아 올리는 것도 할 수 있어?하하 응 그게 묘미더라 물고기 잡혔을 때 ㅎ/...\n",
       "2       야 이새끼 봐바 오타쿠인가봐\\n 뭔데뭔데 아 뭐냐 그림. 이게 그 미미쨩인가 그거냐...\n",
       "3       그랬는데, 내가 몰래 여사친 만나다가 여자친구한테 걸려서... 여자친구가 울면서 헤...\n",
       "4       야.근데.이렇게 하는거 맞아.???\\n맞다고 병신아!!\\n야 근데 왜 자꾸 욕해.?...\n",
       "                              ...                        \n",
       "7945                 아... 어딜 가도 그건 인간 문제임나는 성격부터 죽여야 함...\n",
       "7946    나 진짜 이동하거나 기다릴 때 여름에 땀 나서 못 견디거든아 인정 나 여름에 롯데월...\n",
       "7947                   너는 닮은 연예인 있어?나는 신세경 닮았다는 소리 많이 들었어\n",
       "7948          나는 못하는데 혹시 할 수 있나했지 ㅋㅋ관련 영상 찾아보고 연습해봐야겠다 ㅋㅋ\n",
       "7949                 그래서 빨리 스킨 뽑고 끝내버리고 싶어.무지개큐브 얼마나 모았어?\n",
       "Name: conversation, Length: 7950, dtype: object"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1['conversation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1['conversation'] = data_1['conversation'].str.split('\\n')\n",
    "data_1 = data_1.explode('conversation').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>휴일이니...쉬게해주세요내말이 ... ㅜㅜ 너무 가기 싫어</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>물고기 잡아 올리는 것도 할 수 있어?하하 응 그게 묘미더라 물고기 잡혔을 때 ㅎ/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>야 이새끼 봐바 오타쿠인가봐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>뭔데뭔데 아 뭐냐 그림. 이게 그 미미쨩인가 그거냐?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>하지마.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      class                                       conversation  \\\n",
       "0           0      일반 대화                   휴일이니...쉬게해주세요내말이 ... ㅜㅜ 너무 가기 싫어   \n",
       "1           1      일반 대화  물고기 잡아 올리는 것도 할 수 있어?하하 응 그게 묘미더라 물고기 잡혔을 때 ㅎ/...   \n",
       "2           2  기타 괴롭힘 대화                                    야 이새끼 봐바 오타쿠인가봐   \n",
       "3           2  기타 괴롭힘 대화                      뭔데뭔데 아 뭐냐 그림. 이게 그 미미쨩인가 그거냐?   \n",
       "4           2  기타 괴롭힘 대화                                               하지마.   \n",
       "\n",
       "   labels  \n",
       "0       0  \n",
       "1       0  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data_1['conversation'].tolist()\n",
    "labels = data_1['labels'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41951]\n",
      "그니까 없는 돈 모아서 줄거면 그냥 축의금만 내고 가지 그랬어. 너 밥도 먹었잖아. 돈 아깝게 nan 2 0\n"
     ]
    }
   ],
   "source": [
    "# null 값 확인\n",
    "nan_indices = data_1[data_1['conversation'].isna()].index.tolist()\n",
    "print(nan_indices)\n",
    "\n",
    "print(data_1['conversation'][41950],data_1['conversation'][41951],data_1['labels'][41950], data_1['labels'][41951])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null값 없애기\n",
    "texts.pop(41951)\n",
    "labels.pop(41951)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44912 44912\n"
     ]
    }
   ],
   "source": [
    "print(len(texts), len(labels))  #원래 44913이었음 _제대로 없어짐 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\vocab.json\n",
      "loading file merges.txt from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\merges.txt\n",
      "loading file tokenizer.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"beomi/KcBERT-v2023\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\서우\\.cache\\huggingface\\hub\\models--beomi--KcBERT-v2023\\snapshots\\e35adefea3e730433a3107617d8caa79e0a090ff\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at beomi/KcBERT-v2023 were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at beomi/KcBERT-v2023 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"beomi/KcBERT-v2023\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min length: 0\n",
      "Max length: 268\n",
      "75th percentile: 31.0\n",
      "90th percentile: 45.0\n",
      "95th percentile: 56.0\n",
      "99th percentile: 80.0\n"
     ]
    }
   ],
   "source": [
    "# 원하는 최대 시퀀스 길이 정하기 = 80\n",
    "lengths = [len(text) for text in texts]\n",
    "min_length = np.min(lengths)\n",
    "max_length = np.max(lengths)\n",
    "percentiles = np.percentile(lengths, [75, 90, 95, 99])\n",
    "print(f\"Min length: {min_length}\")\n",
    "print(f\"Max length: {max_length}\")\n",
    "print(f\"75th percentile: {percentiles[0]}\")\n",
    "print(f\"90th percentile: {percentiles[1]}\")\n",
    "print(f\"95th percentile: {percentiles[2]}\")\n",
    "print(f\"99th percentile: {percentiles[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 80\n",
    "labels = torch.tensor(labels, dtype=torch.long)\n",
    "dataset = CustomDataset(texts, labels, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=2024)\n",
    "\n",
    "train_dataloader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **모델 재학습과 평가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\서우\\Desktop\\GitHub\\.venv\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "learning_rate = 0.05\n",
    "epochs = 4\n",
    "\n",
    "# 옵티마이저 및 손실 함수 설정\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[184], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#역전차를 통해 그래디언트 계산\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#옵티마이저를 사용해 가중치 업뎃\u001b[39;00m\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\서우\\Desktop\\GitHub\\.venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\서우\\Desktop\\GitHub\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\서우\\Desktop\\GitHub\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#모델 재학습\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        #그래디언트 초기화\n",
    "        optimizer.zero_grad()\n",
    "        #모델에 입력을 주어 예측을 생성\n",
    "        outputs = model(input_ids, attention_mask = attention_mask)\n",
    "        #모델 출력에서 로짓 점수 얻기 (손실)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        #역전차를 통해 그래디언트 계산\n",
    "        loss.backward()\n",
    "        #옵티마이저를 사용해 가중치 업뎃\n",
    "        optimizer.step()\n",
    "        #에퐄 전체 손실 누적\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    #에퐄별 평균 손실 계산\n",
    "    avg_loss = total_loss/len(train_dataloader)\n",
    "    \n",
    "    #에퐄별 손실 출력\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Avg Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    #모델 평가\n",
    "    model.eval()\n",
    "    val_total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_batch in valid_dataloader:\n",
    "            val_input_ids = val_batch['input_ids']\n",
    "            val_attention_mask = val_batch['attention_mask']\n",
    "            val_labels = val_batch['label']\n",
    "        \n",
    "            # 모델 예측\n",
    "            val_outputs = model(val_input_ids, attention_mask=val_attention_mask)\n",
    "            val_logits = val_outputs.logits\n",
    "\n",
    "            # 손실 계산\n",
    "            val_loss = criterion(val_logits, val_labels)\n",
    "            val_total_loss += val_loss.item()\n",
    "            \n",
    "            # 정확도 계산\n",
    "            val_preds = val_logits.argmax(dim=1)\n",
    "            correct += (val_preds == val_labels).sum().item()\n",
    "            total += val_labels.size(0)\n",
    "    \n",
    "    val_avg_loss = val_total_loss / len(valid_dataloader)\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"Validation Loss: {val_avg_loss:.4f} - Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model_save_path = \"kc_bert__classifier.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "# 모델 아키텍처 생성\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"beomi/KcBERT-v2023\", num_labels=5)\n",
    "\n",
    "# 저장된 가중치 불러오기\n",
    "loaded_model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "loaded_model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = #data\n",
    "\n",
    "input_encodings = tokenizer(input_data, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# 모델에 입력 데이터 전달\n",
    "with torch.no_grad():\n",
    "    output = loaded_model(**input_encodings)\n",
    "    \n",
    "# 예측 결과 확인\n",
    "logits = output.logits\n",
    "predicted_labels = logits.argmax(dim=1)\n",
    "\n",
    "# 예측 결과 출력\n",
    "for i, input_text in enumerate(input_data):\n",
    "    predicted_label = predicted_labels[i].item()\n",
    "    print(f\"Input: {input_text} - Predicted Label: {valid_label(predicted_label)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
